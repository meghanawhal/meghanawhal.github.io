
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">
LatexIT.add('p',true);
</script>
<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:30px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Generating Videos of Zero-Shot Compositions of Actions and Objects</title>
		<meta property="og:image" content="~/projects/zs_hoi_generation/resources/teaser.png"/>
		<meta property="og:title" content="Zero-Shot Generation of Human-Object Interaction Videos." />
		<!-- <meta property="og:description" content="Under Submission." /> -->

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-75863369-6"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-75863369-6');
		</script>
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:34px">Generating Videos of Zero-Shot Compositions of Actions and Objects</span>
	  		  <table align=center width=750px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
                                                    <span style="font-size:18px"><a href="http://www.sfu.ca/~mnawhal/">Megha Nawhal</a> &nbsp;&nbsp;&nbsp;&nbsp; <a href="https://mzhai.weebly.com/">Mengyao Zhai</a> &nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://www.borealisai.com/en/team/andreas-lehrmann/">Andreas Lehrmann</a>  &nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a> &nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://www.cs.sfu.ca/~mori/">Greg Mori</a> </span><br>
		  		  		</center>
		  		  	  </td>

		  		  </tr>
	  			  <tr>
		  		  </tr>
			  </table>
          </center>
          <p style="margin-bottom:0.5cm;"><br></p>
          <center>
  		  <table align=center width=750px>
  			  <tr>
  					<center>
  	                	<img class="round" style="width:500px" src="./zs_hoi_generation/resources/teaser.png"/>
	  				</center>
  	              </td>
  		  </table>
  		</center>

          <hr>


  		  <table align=center width=800px>
	  		  <center><h1>Abstract</h1></center>
	  		  <tr>
	  		  	<td>
Human activity videos involve rich, varied interactions between people and objects. In this paper we develop methods for generating such videos -- making progress toward addressing the important, open problem of video generation in complex scenes.  In particular, we introduce the task of generating human-object interaction videos in a zero-shot compositional setting, \textit{i.e.},  generating videos for action-object compositions that are unseen during training, having seen the target action and target object separately. This setting is particularly important for generalization in human activity video generation, obviating the need to observe every possible action-object combination in training and thus avoiding the combinatorial explosion involved in modeling complex scenes. To generate human-object interaction videos, we propose a novel adversarial framework HOI-GAN which includes multiple discriminators focusing on different aspects of a video. To demonstrate the effectiveness of our proposed framework, we perform extensive quantitative and qualitative evaluation on two challenging datasets: EPIC-Kitchens and 20BN-Something-Something v2.
	  		    </td>
	  		  </tr>
			</table>

<!--  		  <br>
		  <hr>

  		  <table align=center width=450px>
	  		  <center><h1>Method overview</h1></center>
  			  <tr>
  	              <td align=center width=400px>
  					<center>
						  <td><img class="round" style="width:380px" src="./zs_hoi_generation/resources/model_overview.png"/></td>
	  		  		</center>
			  </tr>

		  </center>
		  </table>
  		  <table align=center width=800px>
		  	<center>
		  		<tr>
		  			<td>
                                <p>We propose an adversarial learning scheme in which we train a generator network $\mathbf{G}$ with a set of 4 discriminators: (1) a frame discriminator $\mathbf{D}_f$, which encourages the generator to learn spatially coherent visual content; (2) a gradient discriminator $\mathbf{D}_g$, which incentivizes $\mathbf{G}$ to produce temporally consistent frames; (3) a video discriminator $\mathbf{D}_v$, which provides the generator with global spatio-temporal context; and (4) a relational discriminator $\mathbf{D}_r$, which assists the generator in producing right object layouts in a video. We use pretrained word embeddings for semantic representations of actions and objects. All discriminators are conditioned on word embeddings of the action ($\mathbf{s}_a$) and object ($\mathbf{s}_o$) and trained simultaneously in an end-to-end manner.</p>	
		  			</td>
		  		</tr>
		  </center>
		  </table>
		  <br>
-->
		  <hr>
        <center><h1>Video</h1></center>
        <p align="center">
        <video width="660" height="395" controls>
           <source src="zs_hoi_generation/resources/zs_hoi_generation.mp4" type="video/mp4">
         </video>
      	  <br>
		  <hr>

  		  <!-- <table align=center width=550px> -->
  		  <table align=center width=750px>
	 		<center><h1>Paper and Supplementary Material</h1></center>
  			  <tr>
				  <td><a href="https://arxiv.org/abs/1912.02401"><img class="layered-paper-big" style="height:160px" src="./zs_hoi_generation/resources/paper.png"/></a></td>
				  <td><span style="font-size:12pt">
                                  M. Nawhal, M. Zhai, A. Lehrmann, L. Sigal, G. Mori<br>
				  <b>Generating Videos of Zero-Shot Compositions of Actions and Objects</b><br>
                                  In European Conference on Computer Vision (ECCV), 2020 <br>
				  (hosted on <a href="https://arxiv.org/abs/1912.02401">arXiv</a>)<br>
				  <!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				  <span style="font-size:4pt"><a href=""><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>
		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./zs_hoi_generation/resources/nawhal_eccv20.bib">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>
             <br>
             <hr>
            <br>
        <table style="font-size:11px" align=center>
        <tr>
        <td>
            Template borrowed from <a href='https://richzhang.github.io/colorization/'>Richard Zhang</a>.
        </td>
        </tr>
        </table> 
</body>
</html>
 
